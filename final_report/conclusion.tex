Over the course of this internship project, we employed a broad spectrum of data
analysis and machine learning techniques to develop fast and high-quality
surrogates for a~MC TBR model in use at~UKAEA. We generated over~\num{900000}
samples for training and test purposes, evaluated on this expensive MC~model. We
investigated possibilities for simplification of the parameter space, and
concluded that no straightforward reduction was possible. After reviewing
9~surrogate model families, examining their behaviour on constrained and
unrestricted feature space, and studying their scaling properties, we retrained
some of the best-performing instances to produce properties desirable for
practical use. The best approximator, trained on~\num{500000} datapoints,
featured~$R^2=\num{0.998}$ with mean prediction time
of~$\SI{0.898}{\micro\second}$, representing a relative
speedup of $8\cdot 10^6$ with respect to the MC model. Alternatively, we
also demonstrated the possibility of achieving comparable results using only a
training set of size~\num{10000}.

After a thorough review of the literature, we developed a novel adaptive
sampling algorithm, QASS, capable of interfacing with any of the individual
studied models. Preliminary testing on a toy theory, qualitatively comparable to
the MC TBR model, demonstrated the effectiveness of QASS and behavioural trends
consistent with the design of the algorithm. \textit{[Insert numerical results
for QASS.]} Further optimisation over the hyperparameter space has strong
potential to increase this performance, allowing for future deployment of QASS
on the MC TBR model in coalition with any of the most effective identified
surrogate models.


\section*{Acknowledgements}

The authors would like to thank Vignesh Gopakumar, Prof.~Nikolaos
Konstantinidis, Nikolaos Nikolaou, Prof.~Emily Nurse, Jonathan Shimwell and Ingo
Waldmann for their supervision and valuable suggestions related to this work.

