<<<<<<< HEAD
Over the course of this internship project, we employed a broad spectrum of data analysis and machine learning techniques to develop fast and high-quality surrogate models for a MC TBR model in use at UKAEA. We generated 900,000 samples for training and test purposes, evaluated on this expensive MC model. We investigated possibilities for simplification of the parameter space, and concluded that no straightforward reduction was possible. After reviewing $N$ surrogate models (fill in $N$), and examining their behaviour and scaling properties on the (un)constrained feature space, we retrained some of the best-performing surrogates on the full parameter space. The optimal models obtained had an accuracy of $X$ and a mean prediction time of $X$, representing a relative speedup $X$ with respect to the MC model.
=======
Over the course of this internship project, we employed a broad spectrum of data
analysis and machine learning techniques to develop fast and high-quality
surrogates for a~MC TBR model in use at~UKAEA. We generated over~\num{900000}
samples for training and test purposes, evaluated on this expensive MC~model. We
investigated possibilities for simplification of the parameter space, and
concluded that no straightforward reduction was possible. After reviewing
9~surrogate model families, examining their behaviour on constrained and
unrestricted feature space, and studying their scaling properties, we retrained
some of the best-performing instances to produce properties desirable for
practical use. The best approximator, trained on~\num{500000} datapoints,
featured~$R^2=\num{0.998}$ with mean prediction time
of~$\SI{0.898}{\micro\second}$, representing a relative
speedup~$\num{8659251} \times$ with respect to the MC model. Alternatively, we
also demonstrated the possibility of achieving comparable results using only a
training set of size~\num{10000}.
>>>>>>> a8aa361ad44249d94cd86fe2d10a7f27c74ef277

After a thorough review of the literature, we developed a novel adaptive
sampling algorithm, QASS, capable of interfacing with any of the individual
studied models. Preliminary testing on a toy theory, qualitatively comparable to
the MC TBR model, demonstrated the effectiveness of QASS and behavioural trends
consistent with the design of the algorithm. \textit{[Insert numerical results
for QASS.]} Further optimisation over the hyperparameter space has strong
potential to increase this performance, allowing for future deployment of QASS
on the MC TBR model in coalition with any of the most effective identified
surrogate models.


\section*{Acknowledgements}

The authors would like to thank Vignesh Gopakumar, Prof.~Nikolaos
Konstantinidis, Nikolaos Nikolaou, Prof.~Emily Nurse, Jonathan Shimwell and Ingo
Waldmann for their supervision and valuable suggestions related to this work.

