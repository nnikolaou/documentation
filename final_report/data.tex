The initial step of our work is the study of the existing MC TBR model and its
behaviour. Following the examination of its features (simulation parameters), we
present efficient means of evaluating this model on large sets of points in
high-performance computing (HPC) environment, preprocessing techniques designed
to adapt collected datasets for surrogate modelling, and our attempts at feature
space reduction to achieve the lowest possible number of dimensions.


\subsection{Expensive Model Description}
\label{sec:expensive-model-description}

The expensive MC TBR model is fundamentally a Monte Carlo simulation based on the
OpenMC framework~\cite{ROMANO201590}. As input the software expects 18~parameters, discrete and
continuous, that are fully listed in~\cref{tbl:params}. During evaluation, which
usually takes units of seconds, a fixed number of neutron events is generated, and the
results are given in terms of the mean and the standard deviation of the~TBR aggregated
over the simulated run. The former of these two we accept to be the output TBR
value that is subject to approximation.

\begin{table}[h]
	\centering
	{\footnotesize
		\begin{tabular}{l|llll}
		\toprule
		{} & Parameter Name & Acronym & Type & Domain\\
		\midrule
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{Blanket}}}
		   & Breeder fraction\textsuperscript{\textdagger} & BBF & Continuous & $[0,1]$\\
		   & Breeder \isotope[6]{Li} enrichment fraction & BBLEF & Continuous & $[0,1]$\\
		   & Breeder material & BBM & Discrete & $\{\ce{Li2TiO3}, \ce{Li4SiO4}\}$\\
		   & Breeder packing fraction & BBPF & Continuous & $[0,1]$\\
		   & Coolant fraction\textsuperscript{\textdagger} & BCF & Continuous & $[0,1]$\\
		   & Coolant material & BCM & Discrete & $\{\ce{D2O}, \ce{H2O}, \ce{He}\}$\\
		   & Multiplier fraction\textsuperscript{\textdagger} & BMF & Continuous & $[0,1]$\\
		   & Multiplier material & BMM & Discrete & $\{\ce{Be}, \ce{Be12Ti}\}$\\
		   & Multiplier packing fraction & BMPF & Continuous & $[0,1]$\\
		   & Structural fraction\textsuperscript{\textdagger} & BSF & Continuous & $[0,1]$\\
		   & Structural material & BSM & Discrete & $\{\ce{SiC}, \text{eurofer}\}$\\
		   & Thickness & BT & Continuous & $[0,500]$\\
		\midrule
		\parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{First wall}}}
		   & Armour fraction\textsuperscript{\textdaggerdbl} & FAF & Continuous & $[0,1]$\\
		   & Coolant fraction\textsuperscript{\textdaggerdbl} & FCF & Continuous & $[0,1]$\\
		   & Coolant material & FCM & Discrete & $\{\ce{D2O}, \ce{H2O}, \ce{He}\}$\\
		   & Structural fraction\textsuperscript{\textdaggerdbl} & FSF & Continuous & $[0,1]$\\
		   & Structural material & FSM & Discrete & $\{\ce{SiC}, \text{eurofer}\}$\\
		   & Thickness & FT & Continuous & $[0,20]$\\
		\bottomrule
		\end{tabular}
	}
	\caption{Input parameters supplied to the MC TBR simulation in alphabetical
		order. Groups of fractions marked\textsuperscript{\textdagger
		\textdaggerdbl} are independently required to sum to one.}
	\label{tbl:params}
\end{table}

In the following sections, we often reference TBR points or samples. These are simply vectors
in the feature space generated by Cartesian product of domains of all
features---parameters from~\cref{tbl:params}.

Since most surrogate models
that we employ assume overall continuous numerical inputs, we take steps to unify our
feature interface in order to attain this property. In particular, we transform
discrete features by embedding each such feature using standard one-hot
encoding. This option is available to
us since discrete domains that generate our feature space are finite in
cardinality and relatively small in size. And while it helps us towards
unification, this step comes at the expense of increasing the dimensionality of the
feature space. This is further discussed in~\cref{sec:dimred}.


\subsection{Dataset Generation}
\label{sec:dataset-generation}

In our work, we deliberately make no assumptions about the internal properties of the
MC TBR simulation, effectively treating it as a black box model. This limits our
means of studying its behaviour to inspection of its outputs at various
points in the feature space. We therefore require
sufficiently large and representative quantities of samples to ensure that
surrogates can be trained to approximate the MC TBR model accurately.

With a grid search in such a high-dimensional domain clearly intractable, we
selected uniform pseudo-random sampling\footnote{Continuous and discrete
parameters are drawn from a corresponding uniform distribution over their
domain, as defined in~\cref{tbl:params}. For repeatability, each run uses a seed equal to its number.} to generate large amounts of feature
configurations that we consider to be independent and unbiased. For evaluation
of the expensive MC TBR model, we utilise parallelisation offered by
the HPC infrastructure available at UCL computing facilities. To this end, we
designed and implemented the Approximate TBR Evaluator---a Python software package capable of
sequential evaluation of the multi-threaded OpenMC simulation on batches of
previously generated points in the feature space.
Having deployed ATE at the UCL Hypatia cluster\footnote{The Hypatia RCIF
	partition is comprised of 4~homogeneous nodes. Each node is installed with
	376~GB RAM and 40~Intel\textsuperscript{\textregistered}
	Xeon\textsuperscript{\textregistered} Gold 6148 CPUs of clock frequency
	2.40~GHz.}, we completed three data
generation runs that are summarised in~\cref{tbl:sampling-runs}.

\begin{table}[h]
	\sisetup{round-mode=places,round-precision=2}
	\centering
	{\footnotesize
		\begin{tabular}{rlllll}
		\toprule
		\#	& Samples & Batch division & $t_{\text{run}} $ 
			& $\overline{t}_{\text{eval.}}$ [\si{\second}] & Description \\
		\midrule
		0 & \num{100000} & $\num{100}\times\num{1000}$ & 2~days, 23~h 
		  & $\num{7.877122043981347} \pm \num{2.749485948560108}$ &
		Testing run using old MC TBR version.\\
		1 & \num{500000} & $\num{500}\times\num{1000}$ & 13~days, 20~h
		  & $\num{7.777049573054314} \pm \num{2.8103592103930337}$ &
		Fully uniform sampling in the entire domain.\\
		2 & \num{400000} & $\num{400}\times\num{1000}$ & 10~days 
		  & $\num{7.944379778103232} \pm \num{2.601428571503671}$ &
		Mixed sampling, discrete features fixed.\\
		\bottomrule
		\end{tabular}
	}
	\caption{Parameters of sampling runs. Here, $t_{\text{run}}$ denotes the total run
		time (including waiting in the processing queue), and
		$\overline{t}_{\text{eval.}}$ is the mean evaluation time of the MC TBR
		model (per single sampled point).}
	\label{tbl:sampling-runs}
\end{table}

Skipping run zero, which was performed using an older, fundamentally different
version of the MC TBR software, and was thus treated as a technical
proof-of-concept, we generated a total of~\num{900000} samples in two runs.
While the first run featured fully uniform sampling of the unrestricted feature
space, the second run used a more elaborate strategy. Interested in further study
of relationships between discrete and continuous features, we selected four
assignments of discrete features (listed in~\cref{tbl:slices}) and fixed them
for all points, effectively slicing the feature space into four corresponding
subspaces. In order to achieve comparability, all such \textit{slices} use the
same samples for the values of continuous features.

\begin{table}[h]
	\centering
	{\footnotesize
		\begin{tabular}{rllllll}
		\toprule
		{} & \multicolumn{6}{c}{Discrete feature assignment}\\
		\cmidrule(lr){2-7}
		Batches & BBM & BCM & BMM & BSM & FCM & FSM \\
		\midrule
		0-99 & \ce{Li4SiO4} & \ce{H2O} & \ce{Be12Ti} & eurofer & \ce{H2O} & eurofer\\
		100-199 & \ce{Li4SiO4} & \ce{He} & \ce{Be12Ti} & eurofer & \ce{H2O} & eurofer\\
		200-299 & \ce{Li4SiO4} & \ce{H2O} & \ce{Be12Ti} & eurofer & \ce{He} & eurofer\\
		300-399 & \ce{Li4SiO4} & \ce{He} & \ce{Be12Ti} & eurofer & \ce{He} & eurofer\\
		\bottomrule
		\end{tabular}
	}
	\caption{Selected discrete feature assignments corresponding to slices in run~2.}
	\label{tbl:slices}
\end{table}

Since some surrogate modelling methods applied in this work are not scale-invariant or
perform suboptimally with arbitrarily scaled inputs, all obtained TBR~samples
(features and TBR~values) were standardised prior to further use. In this
commonly used statistical procedure, features and regression outputs are
independently scaled and offset to attain zero mean and unit variance.



\subsection{Dimensionality Reduction}
\label{sec:dimred}

% TODO: here maybe mention desirable effects of dim. red. (e.g. speedup,
% simplification of the problem, better convergence...)

Model training over high-dimensional parameter spaces (illustrated
in~\cref{fig:tbr-vs-features}) may be improved in many
aspects by carefully reducing the number of variables used to describe the
space. For many applications, feature selection strategies succeed in
identifying a sufficiently representative subset of the original input
variables. However, all given variables were assumed to be physically relevant
to the MC TBR model. On the other hand, feature extraction methods aim to
identify a transformation of the parameter space which decreases dimensionality; even if no individual parameter is separable from the space, some linear combinations of parameters or nonlinear functions of parameters may be.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{run1_500k_tbr_vs_feat}
	\caption{Marginalised dependence of TBR on the choice of continuous
	features in \num{500000}~points generated in run~1. Points are coloured by density.}
  \label{fig:tbr-vs-features}
\end{figure}


\subsubsection{Principal Component Analysis}

To pursue linear feature extraction, principal component analysis (PCA)
\cite{Jolliffe2016} was performed via SciKit Learn~\cite{scikit-learn} on a set
of \num{300000} uniform samples of the MC TBR model. % TODO:which run? batch range?

\Cref{fig:pca} shows the resultant cumulative variance of the 11 principal components. The similar share of variance among all features
reveals irreducibility of the TBR model by linear methods.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{fig2_pca.jpg}
	\caption{Cumulative variance for optimal features identified by PCA}
	\label{fig:pca}
\end{figure}

\begin{wrapfigure}[30]{r}{0.5\textwidth}
	\centering
	\hspace*{-.3\columnsep}\includegraphics[width=0.58\textwidth]{fig3_allvar.jpg}
	\caption{Semivariograms for MC TBR data with coolant materials: (a) \ce{He},
	(b) \ce{H2O}, (c) \ce{D2O}}
	\label{fig:var}
\end{wrapfigure}

\subsubsection{Variogram Computations}

Kriging is a geostatistical surrogate modelling technique which relies on
correlation functions over distance (lag) in the feature space~\cite{Bouhlel2018}. Although kriging performed poorly for our use case due to high dimensionality, these correlation measures gave insight into similarities between discrete-parameter slices of the data.

\Cref{fig:var} shows the Matheron semivariance~\cite{Matheron1963} for three discrete slices with coolant material varied, but all other discrete parameters fixed. Fits~\cite{KrigingFig} to the Matérn covariance model confirmed numerically that the coolant material is the discrete parameter with the greatest distinguishability in the MC TBR model. 


\subsubsection{Autoencoders}

Autoencoders~\cite{SCHMIDHUBER201585} are a family of approaches to
dimensionality reduction driven by artificial neural
networks (ANNs). Faced with a broad selection of alternatives, we opted for a
conventional autoencoder architecture with a single hidden layer. While it
follows that the input and output layers of such
network are sized to accommodate the analysed dataset, the
hidden layer, also called \textit{the bottleneck}, allows for variable number of
neurons that represent a smaller subspace. By scanning
over a range of bottleneck widths and investigating relative changes in the
validation loss, we assess the potential for dimensional reduction.

\begin{wrapfigure}[15]{l}{0.25\textwidth}
	\centering
	{\footnotesize \incfigscale{0.8}{autoencoder}}
	\caption{Autoencoder with input width~$W_0$ and bottleneck width~$W_1$.
	Here, $\text{Dense}(N)$ denotes a fully-connected layer of $N$~neurons.}
	\label{fig:autoencoder}
\end{wrapfigure}

In particular, we consider two equally-sized sets\footnote{Each set contained
\num{100000}~samples from batches 0-99 of the corresponding runs.} of samples:
(a)~a~subset of data obtained
from run~1 and (b)~a~subset of a single slice obtained from run~2. Our
expectation was that while the former case would provide meaningful insights into
correlations within the feature space, the latter would validate our
autoencoder implementation by analysing a set of points that are trivially
reducible in dimensionality due to a number of fixed discrete features.

\begin{wrapfigure}[22]{r}{0.4\textwidth}
	\centering
	\vspace{-4ex}

	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth,trim={0 48px 0 0},clip]{ae_mixed}
	\end{subfigure}

	\vspace{-0.2ex}

	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ae_single_slice}
	\end{subfigure}

	\caption{Autoencoder loss scan on the full feature space (top) and a single slice
		(bottom). Dimensional reduction is indicated by a green arrow.}
	\label{fig:autoencoder-loss}
\end{wrapfigure}

The results of both experiments are shown in~\cref{fig:autoencoder-loss}.
Consistent with our motivation, in each plot we can clearly identify a constant
plateau of low error in the region of large dimensionality followed by a point,
from which a steep increase is observed.
We consider this \textit{critical point} to mark the largest viable
dimensional reduction without significant information loss.
With this approach we find that the autoencoder was able to reduce the
datasets into a subspace of 18~dimensions in the first case and 10~dimensions in
the second case.

Confirming our expectation that in the latter, trivial case the
autoencoder should achieve greater dimensional reduction, we are inclined to
believe that our implementation is indeed operating as intended. However, we
must also conclude that in both examined cases this method failed to produce a
reduction that would prove superior to a naïve approach.\footnote{In both tested cases we
	can trivially eliminate 6~dimensions due to overdetermination of one-hot-encoded
	categorical features, and 2~dimensions corresponding to sum-to-one constraints. Furthermore, in the
	single slice case we may omit 6~additional dimensions due to fixed feature
	assignment.} This is consistent with previous results obtained by PCA and kriging.


% TODO: Question: is it accurate to say this represents nonlinear feature extraction? re my intro paragraph

