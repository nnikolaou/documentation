Labeling the expensive MC TBR model $f(x)$, a surrogate is a mapping
$\hat{f}(x)$ such that $f(x)$ and $\hat{f}(x)$ minimise a selected dissimilarity
metric. In order to be considered \textit{viable}, $\hat{f}(x)$ is required to
achieve expected evaluation time lower than that of~$f(x)$. In this work, we
consider two methods of producing viable surrogates: (1) a conventional decoupled
approach, which evaluates $f(x)$ on a set of randomly sampled points and
trains surrogates in a supervised scheme, and (2) an adaptive approach, which attempts to
compensate for localised regression performance insuficiencies by interleaving
multiple epochs of sampling and training.

For both methods, we selected state-of-the-art regression algorithms to perform
surrogate training on sampled point sets. Listed in~\tref{tbl:surrogates}, these
implementations define 9~surrogate families that are later reviewed in~\sref{sec:results}.
We note that each presented algorithm defines hyperparameters that may influence its
performance. Since their optimal values for this problem are unknown, we explore
their assignments prior to other experiments.

\begin{table}[h]
	\caption{\label{tbl:surrogates}Considered surrogate model families.
	$\mathcal{H}$ denotes hyperparameter set.}
	\begin{indented}
	\item[]
		\begin{tabular}{lllr}
		\toprule
		Surrogate & Acronym & Implementation & $|\mathcal{H}|$ \\
		\midrule
		Support vector machines~\cite{fan2008liblinear}	& SVM & SciKit Learn~\cite{scikit-learn} & 3 \\
		Gradient boosted trees~\cite{friedman2001greedy,friedman1999stochastic,hastie2009elements}	& GBT & SciKit Learn & 11 \\
		Extremely randomised trees~\cite{geurts2006extremely}	& ERT & SciKit Learn & 7 \\
		AdaBoosted decision trees~\cite{drucker1997improving}	& ABT & SciKit Learn & 3 \\
		Gaussian process regression~\cite{williams2006gaussian}	& GPR & SciKit Learn & 2 \\
		$k$ nearest neighbours	& KNN & SciKit Learn & 3 \\
		Artificial neural networks	& ANN & Keras (TensorFlow)~\cite{chollet2015keras} & 2 \\
		Inverse distance weighing~\cite{shepard1968two} & IDW & SMT~\cite{SMT2019} & 1 \\
		Radial basis functions & RBF & SMT & 3 \\
		\bottomrule
		\end{tabular}
	\end{indented}
\end{table}

To compare quality of produced surrogates, we define a number of metrics listed
in~\tref{tbl:metrics}. For regression performance analysis, we include a
selection of absolute metrics to assess their approximation capability and set
practical bounds on the expected uncertainty of their predictions. In addition, we also track
relative measures that are better-suited for comparison between this work and others as
they maintain invariance with respect to the selected domain and image space.
For complexity analysis, surrogates are assessed in terms of wall
time (captured by the Python~\texttt{time} package). This is motivated by common practical use
cases of our work, where models are trained and used as drop-in replacements for the
expensive MC TBR model. Since training set sizes remain to be determined, all times are
reported per a single datapoint. Even though some surrogates support acceleration
by means of parallelisation, sequential processing of samples was ensured to
achieve comparability between considered models. The only exception to this are
ANNs, which require considerable amount of processing power for training on
conventional CPU architectures. Lastly, to prevent undesirable bias by training set
selection, all metrics are collected in the scheme of 5-fold cross-validation.

\begin{table}[h]
	\caption{\label{tbl:metrics}Metrics recorded in experiments. In
	formulations, we work with a training set of size $N_0$ and a test set of
size $N$, values $y^{(i)}=f(x^{(i)})$ and $\hat{y}^{(i)}=\hat{f}(x^{(i)})$
denote images of the $i$th testing sample in the MC TBR model and the surrogate
respectively. Furthermore, the mean $\overline{y}=\sum_{i=1}^N y^{(i)}/N$ and $P$ is the
number of input features.}
	\begin{indented}
	\item[]
		\begin{tabular}{lrl}
		\toprule
		Regression perf. metrics& Notation	& Mathematical formulation\\
		\midrule
		Mean absolute error	& MAE & $\sum_{i=1}^N |y^{(i)}-\hat{y}^{(i)}|/N$ \\
		Standard error of regression & $S$	& $\text{StdDev}_{i=1}^N\left\{ |y^{(i)} -
		\hat{y}^{(i)}| \right\} $ \\
			Coefficient of determination & $R^2$	& $1-\sum_{i=1}^N \left(y^{(i)}-\hat{y}^{(i)} \right)^2 /
		\sum_{i=1}^N \left( y^{(i)}-\overline{y} \right)^2 $ \\
			Adjusted $R^2$ & $R^2_\text{adj.}$	& $1-(1-R^2)(N-1)/(N-P-1)$ \\
		\midrule
		Complexity metrics	& {}	& {} \\
		\midrule
		Mean training time & $\overline{t}_{\text{trn.}}$	& $(\text{wall training time of
		$\hat{f}(x)$})/N_0$  \\
			Mean prediction time & $\overline{t}_{\text{pred.}}$	& $(\text{wall prediction time of
		$\hat{f}(x)$})/N$ \\
				Relative speedup & $\omega$	& $(\text{wall evaluation time of $f(x)$}) /
		(N\overline{t}_{\text{pred.}})$ \\
		\bottomrule
		\end{tabular}
	\end{indented}
\end{table}



\subsection{Decoupled Sampling}
\label{sec:experiment-methodology}
\todo{This section needs to be shortened}

The presented surrogate candidates are evaluated in four experimental cases:%
\begin{enumerate}
	\item Hyperparameter tuning in a simplified domain.

	\item Hyperparameter tuning in full domain.

	\item Scaling benchmark.

	\item Model comparison.
\end{enumerate}

The aim of the initial experiments is to use a relatively small subset of
collected TBR samples to determine hyperparameters of considered surrogates.
Since this process requires learning the behaviour of an unknown, possibly
expensive mapping -- here a function that assigns cross-validated metrics to a
point in the hyperparameter domain -- it in many aspects mirrors
the primary task of this work with the notable extension of added utility
to optimise. In order to avoid undesirable exponential slowdown in exhaustive
searches of a possibly high-dimensional parameter space, Bayesian
optimisation~\cite{movckus1975bayesian} is employed as a standard hyperparameter tuning algorithm. We set
its objective to maximise $R^2$ and perform
1000~iterations.\footnote{Hyperparameter tuning of each surrogate family was
	terminated after 2~days. Instances that reached this limit may be identified
	in Table~\ref{tbl:exp1-detailed-results,tbl:exp2-detailed-results} in the
	Appendix.}

In the first experiment, efforts are made to maximise the possibility of success
in surrogates that are prone to suboptimal performance in discontinuous spaces.
This follows the notion that, if desired, performance of such models may be
replicated by training separate instances to model each continuous subregion of
the domain independently.
To this end, data are limited to a single slice from run~2, and discrete
features are completely withheld from evaluated
surrogates. This is repeated for each of the four available slices to
investigate variance in behaviour under different discrete feature assignments.
The second experiment conventionally measures surrogate performance on the full
feature space. Here, in extension of the previous case, surrogates work with
samples comprised of discrete as well as continuous features.

The objective of the last two experiments is to exploit the information gathered by
hyperparameter tuning. In the third experiment, the 20~best-performing
hyperparameter configurations of each family (with respect to~$R^2$) are used to
perform training on progressively larger sets to investigate their scaling
properties. Following that, the fourth experiment aims to produce surrogates
suitable for practical use by retraining selected well-scaling instances on large
training sets to satisfy the goals of this work.

\subsection{Adaptive Sampling}
\label{sec:adaptive}
\todo{This section needs to be shortened}

\begin{figure}[h]
	\centering
	Placeholder % TODO: \includegraphics[width=0.7\textwidth]{fig4_qassplan.png}
	\caption{Schematic of QASS algorithm}
	\label{fig:qassplan}
\end{figure}

All of the surrogate modelling techniques studied in this project face a common
challenge: their accuracy is limited by the quantity of training samples which
are available from the expensive MC TBR model. Adaptive sampling procedures can
improve upon this limitation by taking advantage of statistical information
which is accumulated during the training of any surrogate model. Rather than
training the surrogate on a single sample set generated according to a fixed
strategy, sample locations are chosen periodically during training so as to best suit the model
under consideration.

Adaptive sampling techniques appear frequently in the literature and have been
specialised for surrogate modelling. Garud's~\cite{Garud2016} ``Smart Sampling
Algorithm'' achieved notable success by incorporating surrogate quality and
crowding distance scoring to identify optimal new samples, but was only tested
on a single-parameter domain. We theorised that a nondeterministic sample
generation approach, built around Markov Chain Monte Carlo methods (MCMC), would
fare better for high-dimensional models by more thoroughly exploring all local
optima in the feature space. MCMC produces a progressive chain of sample points,
each drawn according to the same symmetric proposal distribution\footnote{An
adaptive MCMC procedure~\cite{Zhang2012}, which adjusts an ellipsoidal proposal
distribution to fit the posterior, was also implemented but not fully tested.}
from the prior point. These sample points will converge to a desired posterior
distribution, so long as the acceptance probability for these draws has a
particular functional dependence on that posterior value (see~\cite{Zhou2018}
for a review).


Many researchers have embedded surrogate methods into MCMC strategies for
parameter optimisation~\cite{Zhang2020,Gong2017}, in particular the ASMO-PODE
algorithm~\cite{Ginting2011} which makes use of MCMC-based adaptive sampling to
attain greater surrogate precision around prospective optima. Our novel approach
draws inspiration from ASMO-PODE, but instead uses MCMC to generate samples
which increase surrogate precision throughout the entire parameter space.

We designed the Quality-Adaptive Surrogate Sampling algorithm (QASS,
Figure~\ref{fig:qassplan}) to iteratively increment the training/test set with sample
points which maximise surrogate error and minimise a crowding distance metric
(CDM)~\cite{Solonen2012} in feature space. On each iteration following an initial training of the surrogate on $N$ uniformly random samples, the surrogate was trained and absolute error calculated. MCMC was then performed on the error function generated by performing nearest-neighbor interpolation on these test error points. The resultant samples were culled by $50\%$ according to the CDM, and then the $n$ highest-error candidates were selected for reintegration with the training/test set, beginning another training epoch. Validation was also performed during each iteration on independent, uniformly-random sample sets.

% Adaptive multi-chain MCMC:~\cite{Zhang2012}
